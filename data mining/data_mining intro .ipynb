{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data has 500 rows and 9241 columns \n",
      "number of mislabels out of 125 points: 14\n"
     ]
    }
   ],
   "source": [
    "# import newsGroups data\n",
    "import sys\n",
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='train',\n",
    "                          categories=('rec.autos',\n",
    "                             'rec.sport.hockey'),\n",
    "                          remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "#generate term frequency matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vec = CountVectorizer (max_df=500, \n",
    "                      min_df=0,\n",
    "                      max_features =10000, \n",
    "                      ngram_range =(1,1),\n",
    "                     stop_words='english')\n",
    "tf_matrix=tf_vec.fit_transform(news.data[:500])  #sparse matrix\n",
    "print (\"the data has %d rows and %d columns \" % (tf_matrix.shape[0], tf_matrix.shape[1]))\n",
    "\n",
    "import pandas as pd             #conver to full matrix\n",
    "full_matrix = pd.DataFrame(tf_matrix.todense(),columns=tf_vec.get_feature_names())\n",
    "#print (full[1:10]) \n",
    "\n",
    "\n",
    "\n",
    "#classify data\n",
    "t=np.asarray(news.target[:500])   # true labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=50) \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "clf = NB()\n",
    "#from sklearn import tree\n",
    "#clf = tree.DecisionTreeClassifier()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of %d points: %d\" % (xtest.shape[0],error ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  is 0.89 \n",
      "precision is 0.85 \n",
      "recall    is 0.91 \n",
      "F1-score  is 0.88 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "print(\"accuracy  is %2.2f \" % (metrics.accuracy_score(ytest, y_pred)))\n",
    "print(\"precision is %2.2f \" % (metrics.precision_score(ytest, y_pred)))\n",
    "print(\"recall    is %2.2f \" % (metrics.recall_score(ytest,y_pred)))\n",
    "print(\"F1-score  is %2.2f \" % (metrics.f1_score(ytest,y_pred)))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Robust Model\n",
    "\n",
    "*   Split data into training and test sets using the following techniques:\n",
    "-\n",
    "    - hold-out method \n",
    "    - cross validation\n",
    "    - bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n",
      "0.92\n",
      "0.92\n",
      "0.92\n",
      "0.92\n",
      "0.92\n",
      "0.92\n",
      "0.92\n",
      "0.92\n",
      "0.92\n",
      "Average Accuracy is 0.920\n"
     ]
    }
   ],
   "source": [
    "# Hold-out method\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "E=[]\n",
    "no_iter=10\n",
    "for i in range (0,no_iter): \n",
    "   xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=20) \n",
    "   clf = NB()\n",
    "   y = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "   acc=metrics.accuracy_score(ytest, y)\n",
    "   E.append(acc)\n",
    "   print (acc)\n",
    "print (\"Average Accuracy is %3.3f\" % (sum(E)/no_iter) ) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of fold (1) is 0.920\n",
      " Accuracy of fold (2) is 0.940\n",
      " Accuracy of fold (3) is 0.980\n",
      " Accuracy of fold (4) is 0.840\n",
      " Accuracy of fold (5) is 0.940\n",
      " Accuracy of fold (6) is 0.940\n",
      " Accuracy of fold (7) is 0.960\n",
      " Accuracy of fold (8) is 0.880\n",
      " Accuracy of fold (9) is 0.860\n",
      " Accuracy of fold (10) is 0.880\n"
     ]
    }
   ],
   "source": [
    "# K-fold Cross Validation \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "t=np.asarray(news.target[:500])   # true labels\n",
    "kf = KFold(n_splits =10)\n",
    "i=0\n",
    "for train, test in kf.split(tf_matrix): \n",
    "    xtrain,xtest = tf_matrix[train],  tf_matrix[test]\n",
    "    ytrain, ytest = t[train], t[test]\n",
    "    clf = NB()\n",
    "    y = clf.fit(xtrain.toarray(), ytrain).predict(xtest.toarray())\n",
    "    acc=metrics.accuracy_score(ytest, y)\n",
    "    i=i+1\n",
    "\n",
    "    print (\" Accuracy of fold (%d) is %3.3f\" % (i,acc ))  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Enhancement\n",
    "- Bagging\n",
    "- Boosting\n",
    "- Ensemble\n",
    "    * Random Forest\n",
    "    * AdaBost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Bagging - number of mislabels out of 125 points: 9\n",
      "Witout Bagging - number of mislabels out of 125 points: 14\n",
      "With AdaBoost - number of mislabels out of 125 points: 14\n"
     ]
    }
   ],
   "source": [
    "# -------------  Naive Bayes Classifier  --------------#\n",
    "\n",
    "# Bagging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "from sklearn import tree \n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=50)\n",
    "\n",
    "bagging = BaggingClassifier (NB(), max_samples=.5, max_features=.5)\n",
    "y_pred = bagging.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"With Bagging - number of mislabels out of %d points: %d\" % (xtest.shape[0],error ))\n",
    "\n",
    "\n",
    "# NO bagging\n",
    "clf = NB()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"Witout Bagging - number of mislabels out of %d points: %d\" % (xtest.shape[0],error ))\n",
    "\n",
    "\n",
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(NB(),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=200)\n",
    "\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"With AdaBoost - number of mislabels out of %d points: %d\" % (xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Witout Bagging - number of mislabels out of 125 points: 24\n",
      "With Bagging - number of mislabels out of 125 points: 22\n",
      "With Random Forest- number of mislabels out of 125 points: 13\n",
      "With AdaBoost - number of mislabels out of 125 points: 17\n"
     ]
    }
   ],
   "source": [
    "# ------ DECISION TREES --------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree \n",
    "\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=50)\n",
    "\n",
    "\n",
    "# NO bagging\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"Witout Bagging - number of mislabels out of %d points: %d\" % (xtest.shape[0],error ))\n",
    "\n",
    "\n",
    "#Bagging\n",
    "bagging = BaggingClassifier (tree.DecisionTreeClassifier(), )\n",
    "y_pred = bagging.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"With Bagging - number of mislabels out of %d points: %d\" % (xtest.shape[0],error ))\n",
    "\n",
    "#random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=None,random_state=10, max_features='auto')\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"With Random Forest- number of mislabels out of %d points: %d\" % (xtest.shape[0],error ))\n",
    "\n",
    "\n",
    "# AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=2),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=200)\n",
    "\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"With AdaBoost - number of mislabels out of %d points: %d\" % (xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multi-Class Classification\n",
    "- One-vs-All\n",
    "- One-vs-One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data has 2381 rows and 29250 columns \n",
      "One-vs-All --> number of mislabels out of 596 points in the test test: 94\n",
      "One-vs-One --> number of mislabels out of 596 points in the test test: 106\n",
      "Kmeans --> number of mislabels out of 1785 points in the test test: 1346\n"
     ]
    }
   ],
   "source": [
    "# Mutli-calss classification\n",
    "\n",
    "# import newsGroups data\n",
    "import sys\n",
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='train',\n",
    "                          categories=('rec.autos',\n",
    "                             'rec.sport.hockey',\n",
    "                             'sci.med',\n",
    "                             'sci.space'\n",
    "                                     ),\n",
    "                          remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "#generate term frequency matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vec = CountVectorizer (max_df=500, \n",
    "                      min_df=0,\n",
    "                      max_features =30000, \n",
    "                      ngram_range =(1,1),\n",
    "                     stop_words='english')\n",
    "\n",
    "tf_matrix=tf_vec.fit_transform(news.data)  #sparse matrix\n",
    "print (\"the data has %d rows and %d columns \" % (tf_matrix.shape[0], tf_matrix.shape[1]))\n",
    "\n",
    "import pandas as pd             #conver to full matrix\n",
    "full_matrix = pd.DataFrame(tf_matrix.todense(),columns=tf_vec.get_feature_names())\n",
    "\n",
    "\n",
    "#One-vs-All (one-vs-Rest)\n",
    "import numpy as np\n",
    "t=np.asarray(news.target)   # true labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=50) \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf= LinearSVC(random_state=10)\n",
    "#clf = NB()\n",
    "\n",
    "y_pred = OneVsRestClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"One-vs-All --> number of mislabels out of %d points in the test test: %d\" % (xtest.shape[0],error ))\n",
    "\n",
    "\n",
    "# One-vs-One (All-vs-All)\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "clf= LinearSVC(random_state=10)\n",
    "#clf = NB()\n",
    "\n",
    "y_pred = OneVsOneClassifier(clf).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"One-vs-One --> number of mislabels out of %d points in the test test: %d\" % (xtest.shape[0],error ))\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(xtrain)\n",
    "error = (kmeans.labels_ != ytrain).sum()\n",
    "print (\"Kmeans --> number of mislabels out of %d points in the test test: %d\" % (xtrain.shape[0],error ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning (Clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans --> number of mislabels out of 1785 points in the data: 1346\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(xtrain)\n",
    "error = (kmeans.labels_ != ytrain).sum()\n",
    "print (\"Kmeans --> number of mislabels out of %d points in the data: %d\" % (xtrain.shape[0],error ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
